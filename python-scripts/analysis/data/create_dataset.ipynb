{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset creation for sound localisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from data.utils import *\n",
    "from audio_utils import filter_voice, get_fft_gram, get_fbanks_gcc\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters for the creation of the dataset\n",
    "INPUT_LENGTH = 1000\n",
    "INPUT_TYPE = \"gammagram\"\n",
    "CSV_DATA = \"./sound_angles.csv\"\n",
    "CHUNCK_OVERLAP = 150\n",
    "OUTPUT_DIR = \"dataset_\"+ str(INPUT_LENGTH)\n",
    "RESAMPLING_F = 0\n",
    "THRESHOLD_VOICE = 60\n",
    "SAVE_RAW = False\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"data\"))\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"raw\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process the audio files and create chuncks of audio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1186.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d36d11bd90140c08ba090342a4d243f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 30820 samples\n"
     ]
    }
   ],
   "source": [
    "def get_feature(signal, fs):\n",
    "    if INPUT_TYPE == \"gammagram\":\n",
    "        return get_fft_gram(signal, fs, time_window=0.015, channels=128, freq_min=120)\n",
    "    elif INPUT_TYPE == \"fbank_gcc\":\n",
    "        signal = np.transpose(signal)\n",
    "        return get_fbanks_gcc(signal, fs,  win_size=1024, hop_size=512, nfbank=50)\n",
    "\n",
    "data_csv = pd.read_csv(CSV_DATA)\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "file_cpt = 0\n",
    "for index, item in tqdm(data_csv.iterrows(), total=data_csv.shape[0]):\n",
    "    audio_filename = item['audio_filename']\n",
    "    end_audio = item['stop_audio_timestamp'] - item['start_audio_timestamp']\n",
    "    sample_rate, chunks_channel1, chunks_channel2 = split_audio_chunks(audio_filename, end_audio,\n",
    "                                                                       size_chunks=INPUT_LENGTH,\n",
    "                                                                       overlap=CHUNCK_OVERLAP)\n",
    "\n",
    "    for j, (signal1, signal2) in enumerate(zip(chunks_channel1, chunks_channel2)):\n",
    "\n",
    "        signal1 = librosa.util.normalize(signal1 / 32768.0)\n",
    "        signal2 = librosa.util.normalize(signal2 / 32768.0)\n",
    "\n",
    "        if filter_voice(signal1, sample_rate, threshold=THRESHOLD_VOICE):\n",
    "            filename = str(file_cpt) + '.npy'\n",
    "\n",
    "            if RESAMPLING_F:\n",
    "                    signal1 = np.array(scipy.signal.resample(signal1, RESAMPLING_F))\n",
    "                    signal2 = np.array(scipy.signal.resample(signal2, RESAMPLING_F))\n",
    "                    sample_rate = RESAMPLING_F\n",
    "\n",
    "            data = np.stack((signal1, signal2), axis=1)\n",
    "            feat = get_feature(data, sample_rate)\n",
    "\n",
    "            np.save(os.path.join(OUTPUT_DIR, \"data\",filename), feat)\n",
    "            if SAVE_RAW:\n",
    "                filename_raw = str(file_cpt) + '.wav'\n",
    "                scipy.io.wavfile.write(os.path.join(OUTPUT_DIR, \"raw\",filename), sample_rate, data)\n",
    "\n",
    "            new_df = new_df.append(item, ignore_index=True)\n",
    "            new_df.at[file_cpt, 'audio_filename'] = filename\n",
    "            file_cpt += 1\n",
    "\n",
    "new_df.to_csv(os.path.join(OUTPUT_DIR, \"dataset.csv\"), index=False)\n",
    "\n",
    "print(f\"Created {file_cpt} samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process the audio files and create chuncks of audio background\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2439.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32e9d55260d54c048d44fd1a6e6479b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 2439 background samples\n"
     ]
    }
   ],
   "source": [
    "background_file = \"/home/icub/Documents/Jonas/dataset/data/background/mix_bg.wav\"\n",
    "\n",
    "df = pd.DataFrame(columns=['audio_filename', 'labels'])\n",
    "\n",
    "sample_rate, chunks_channel1, chunks_channel2 = split_audio_chunks(background_file, None,\n",
    "                                                                   size_chunks=INPUT_LENGTH,\n",
    "                                                                   overlap=INPUT_LENGTH)\n",
    "cpt = 0\n",
    "for (audio_c1, audio_c2) in tqdm(zip(chunks_channel1, chunks_channel2), total=len(chunks_channel2)):\n",
    "    filename = f\"{cpt}_bg.npy\"\n",
    "\n",
    "    audio_c1 = librosa.util.normalize(audio_c1 / 32768.0)\n",
    "    audio_c2 = librosa.util.normalize(audio_c2 / 32768.0)\n",
    "\n",
    "    if RESAMPLING_F:\n",
    "        audio_c1 = np.array(scipy.signal.resample(audio_c1, RESAMPLING_F))\n",
    "        audio_c2 = np.array(scipy.signal.resample(audio_c2, RESAMPLING_F))\n",
    "        sample_rate = RESAMPLING_F\n",
    "\n",
    "    data = np.stack((audio_c1, audio_c2), axis=1)\n",
    "    feat = get_feature(data, sample_rate)\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"data\",filename), feat)\n",
    "    cpt += 1\n",
    "    df = df.append({\"audio_filename\": filename, \"labels\": -1}, ignore_index=True)\n",
    "\n",
    "\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, f\"background.csv\"), index=False)\n",
    "print(f\"Created {cpt} background samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}